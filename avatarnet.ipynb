{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['gpu_device'] = 0\n",
    "\n",
    "params['train_flag'] = True\n",
    "params['max_iteration'] = 80000\n",
    "params['batch_size'] = 8\n",
    "params['layers'] = [1, 6, 11, 20]\n",
    "params['feature_loss_weight'] = 0.1\n",
    "params['reconstruction_loss_weight'] = 1\n",
    "params['tv_loss_weight'] = 1\n",
    "params['imsize'] = 512\n",
    "params['cropsize'] = 256\n",
    "params['lr'] = 1e-3\n",
    "\n",
    "params['data_path'] = '../coco2014/train2014_512/'\n",
    "params['file_name'] = '../coco2014/train.txt'\n",
    "\n",
    "params['save_path'] = 'trained_models/'\n",
    "params['load_path'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from style_decorator import StyleDecorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(params['gpu_device'])\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std of imagenet for pre-trained VGG network\n",
    "# ref: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "normalize = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "denormalize = transforms.Normalize(mean=[-mean/std for mean,std in zip(imagenet_mean, imagenet_std)],\n",
    "                                   std=[1/std for std in imagenet_std])\n",
    "\n",
    "pil2tensor = transforms.ToTensor()\n",
    "tensor2pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, root_path, file_name, transform=None):\n",
    "        # Image will be aranged in this way: root_dir/image1.png        \n",
    "        self.root_path = root_path\n",
    "        self.imlist = [fname.strip() for fname in open(file_name).readlines()]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def image_loader(self, path):\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.image_loader(self.root_path+self.imlist[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imload(path, imsize=512, cropsize=512):\n",
    "    transformer = []\n",
    "    if imsize:\n",
    "        transformer.append(transforms.Resize(imsize))\n",
    "    if cropsize:\n",
    "        transformer.append(transforms.CenterCrop(cropsize))\n",
    "    transformer.append(transforms.ToTensor())\n",
    "    transformer.append(normalize)    \n",
    "    transformer = transforms.Compose(transformer)\n",
    "    \n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    return transformer(image).unsqueeze(0)\n",
    "\n",
    "def imshow(tensor):\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()        \n",
    "    tensor = torchvision.utils.make_grid(tensor)\n",
    "    tensor = denormalize(tensor)\n",
    "    tensor.clamp_(0.0, 1.0)\n",
    "    image = tensor2pil(tensor)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, x, layer_indices):\n",
    "    features = []\n",
    "    y = x.clone()\n",
    "    for i, layer in enumerate(model):\n",
    "        y = layer(y)\n",
    "        if i in layer_indices:\n",
    "            features.append(y)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg encoder/decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(vgg, layers):\n",
    "    encoder = nn.ModuleList()\n",
    "    temp_seq = nn.Sequential()\n",
    "    for i in range(max(layers)+1):\n",
    "        temp_seq.add_module(str(i), vgg[i])\n",
    "        if i in layers:\n",
    "            encoder.append(temp_seq)\n",
    "            temp_seq = nn.Sequential()\n",
    "            \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(vgg, layers):\n",
    "    decoder = nn.ModuleList()\n",
    "    temp_seq  = nn.Sequential()\n",
    "    count = 0\n",
    "    for i in range(max(layers)-1, -1, -1):\n",
    "        if isinstance(vgg[i], nn.Conv2d):\n",
    "            out_channels = vgg[i].in_channels\n",
    "            in_channels = vgg[i].out_channels\n",
    "            kernel_size = vgg[i].kernel_size\n",
    "\n",
    "            temp_seq.add_module(str(count), nn.ReflectionPad2d(padding=(1,1,1,1)))\n",
    "            count += 1\n",
    "            temp_seq.add_module(str(count), nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size))\n",
    "            count += 1\n",
    "            temp_seq.add_module(str(count), nn.ReLU())\n",
    "            count += 1\n",
    "        elif isinstance(vgg[i], nn.MaxPool2d):\n",
    "            temp_seq.add_module(str(count), nn.Upsample(scale_factor=2))\n",
    "            count += 1\n",
    "\n",
    "        if i in layers:\n",
    "            decoder.append(temp_seq)\n",
    "            temp_seq  = nn.Sequential()\n",
    "\n",
    "    # append last conv layers without ReLU activation\n",
    "    decoder.append(temp_seq[:-1])    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avatar Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvatarNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(AvatarNet, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).features\n",
    "        \n",
    "        self.encoders = get_encoder(vgg, layers)\n",
    "        self.decoders = get_decoder(vgg, layers)\n",
    "        \n",
    "        self.adain = AdaIN()\n",
    "        self.decorator = StyleDecorator()\n",
    "        \n",
    "    def forward(self, c, s, train_flag=False):\n",
    "        \n",
    "        # encode content image\n",
    "        for encoder in self.encoders:\n",
    "            c = encoder(c)\n",
    "        \n",
    "        # encode style image\n",
    "        features = []\n",
    "        for encoder in self.encoders:\n",
    "            s = encoder(s)\n",
    "            features.append(s)\n",
    "        del features[-1]\n",
    "        \n",
    "        if not train_flag:\n",
    "            c = self.decorator(c, [s])\n",
    "        \n",
    "        # decode bottleneck feature        \n",
    "        for decoder in self.decoders:\n",
    "            c = decoder(c)\n",
    "            if features:\n",
    "                c = self.adain(c, features.pop())\n",
    "            \n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(AdaIN, self).__init__()\n",
    "        \n",
    "    def forward(self, x, t, eps=1e-5):\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        x_mean = torch.mean(x.view(b, c, h*w), dim=2, keepdim=True)\n",
    "        x_std = torch.std(x.view(b, c, h*w), dim=2, keepdim=True)\n",
    "        \n",
    "        t_b, t_c, t_h, t_w = t.size()\n",
    "        t_mean = torch.mean(t.view(t_b, t_c, t_h*t_w), dim=2, keepdim=True)\n",
    "        t_std = torch.std(t.view(t_b, t_c, t_h*t_w), dim=2, keepdim=True)\n",
    "        \n",
    "        x_ = ((x.view(b, c, h*w) - x_mean)/(x_std + eps))*t_std + t_mean\n",
    "        \n",
    "        return x_.view(b, c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCalculator:\n",
    "    def __init__(self, layers, feature_loss_weight, reconstruction_loss_weight, tv_loss_weight):\n",
    "        self.loss_network = torchvision.models.vgg19(pretrained=True).features\n",
    "        self.loss_network = self.loss_network.cuda()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        self.feature_loss_weight = feature_loss_weight\n",
    "        self.reconstruction_loss_weight = reconstruction_loss_weight\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "        \n",
    "        self.mse_criterion = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        self.loss_seq = dict()\n",
    "        self.loss_seq['total_loss'] = []\n",
    "        self.loss_seq['feature_loss'] = []\n",
    "        self.loss_seq['reconstruction_loss'] = []\n",
    "        self.loss_seq['tv_loss'] = []\n",
    "        \n",
    "    def calc_total_loss(self, output, target):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # reconstruction loss\n",
    "        reconstruction_loss = self.mse_criterion(output, target)\n",
    "        self.loss_seq['reconstruction_loss'].append(reconstruction_loss.item())\n",
    "        total_loss += reconstruction_loss * self.reconstruction_loss_weight\n",
    "        \n",
    "        # feature loss\n",
    "        output_features = extract_features(self.loss_network, output, self.layers)\n",
    "        target_features = extract_features(self.loss_network, target, self.layers)\n",
    "        feature_loss = 0\n",
    "        for output_feature, target_feature in zip(output_features, target_features):\n",
    "            feature_loss+= self.mse_criterion(output_feature, target_feature) * 1/len(output_features)    \n",
    "        self.loss_seq['feature_loss'].append(feature_loss.item())\n",
    "        total_loss += feature_loss * self.feature_loss_weight\n",
    "        \n",
    "        # tv loss\n",
    "        tv_loss = self.calc_tv_loss(output)\n",
    "        self.loss_seq['tv_loss'].append(tv_loss.item())\n",
    "        total_loss += tv_loss * self.tv_loss_weight\n",
    "        \n",
    "        self.loss_seq['total_loss'].append(total_loss.item())\n",
    "        return total_loss\n",
    "                \n",
    "    \n",
    "    def calc_tv_loss(self, x):        \n",
    "        return  torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])) + torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
    "\n",
    "    def print_loss_seq(self):\n",
    "        str_ = '%s: '%time.ctime()\n",
    "        for key, value in self.loss_seq.items():\n",
    "            if len(value) > 100:\n",
    "                length = 100\n",
    "            else:\n",
    "                length = 1\n",
    "            str_ += '%s: %2.4f,\\t'%(key, sum(value[-length:])/length)\n",
    "        print(str_)\n",
    "        \n",
    "    def draw_loss_seq(self):\n",
    "        for key, value in self.loss_seq.items():\n",
    "            plt.semilogy(value, label=key)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avatarnet = AvatarNet(layers=params['layers'])\n",
    "avatarnet = avatarnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_calculator = LossCalculator(layers=params['layers'],  \n",
    "                                 feature_loss_weight=params['feature_loss_weight'], \n",
    "                                 reconstruction_loss_weight=params['reconstruction_loss_weight'], \n",
    "                                 tv_loss_weight=params['tv_loss_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['train_flag']:\n",
    "    data_set = ImageFolder(params['data_path'], params['file_name'], transform=transforms.Compose([\n",
    "        transforms.Resize(params['imsize']),\n",
    "        transforms.RandomCrop(params['cropsize']),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "    optimizer = optim.Adam(params=avatarnet.decoders.parameters(), lr=params['lr'])\n",
    "\n",
    "    for iteration in range(params['max_iteration']):\n",
    "        data_loader = data.DataLoader(data_set, batch_size=params['batch_size'], shuffle=True)\n",
    "        image = next(iter(data_loader)).cuda()\n",
    "\n",
    "        output = avatarnet(image, image, train_flag=True)\n",
    "        total_loss = loss_calculator.calc_total_loss(output, image)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()                \n",
    "\n",
    "        if (iteration+1) % 1000 == 0:\n",
    "            loss_calculator.print_loss_seq()\n",
    "            loss_calculator.draw_loss_seq()\n",
    "            torch.save(avatarnet.state_dict(), params['save_path']+'avatarnet.pth')    \n",
    "            print(\"output\")\n",
    "            imshow(output.data)\n",
    "            print(\"input\")\n",
    "            imshow(image.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['load_path']:\n",
    "    avatarnet.load_state_dict(torch.load(params['load_path'] + 'avatarnet.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stylize a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = imload('../test-image-dataset/content-images/brad_pitt.jpg', imsize=params['imsize']).cuda()\n",
    "style_image = imload('../test-image-dataset/style-images/mondrian.jpg', imsize=params['imsize']).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = avatarnet(content_image, style_image, train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imshow(content_image.data)\n",
    "imshow(style_image.data)\n",
    "imshow(output.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-1.1",
   "language": "python",
   "name": "pytorch-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
